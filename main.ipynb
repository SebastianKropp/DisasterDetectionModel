{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Disaster Detection Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.23.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.39.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (9.5.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (5.12.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (3.12.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from torch->-r requirements.txt (line 5)) (3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 3)) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 5)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\elmig\\miniconda3\\lib\\site-packages (from sympy->torch->-r requirements.txt (line 5)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip3 install -r requirements.txt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "import requests\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "###              EXAMPLE USE:                ###\n",
    "## tweet_text = \"This is a test tweet!\"\n",
    "## sentiment = sentiment_detection(tweet_text)\n",
    "## sarcasm = sarcasm_detection(tweet_text) \n",
    "API_TOKEN = \"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\"\n",
    "\n",
    "def query(API_URL, headers, payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "def sentiment_detection(tweet_text):\n",
    "    # Define the first API endpoint and function\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    # Use the first function to query the sentiment of some text\n",
    "    output_sentiment = query(API_URL, headers, {\n",
    "        \"inputs\": tweet_text,\n",
    "    })\n",
    "\n",
    "    return output_sentiment\n",
    "\n",
    "\n",
    "def sarcasm_detection(tweet_text):\n",
    "    # Define the second API endpoint and function\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/helinivan/english-sarcasm-detector\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "        \n",
    "    output_sarcasm = query(API_URL, headers, {\n",
    "        \"inputs\": tweet_text,\n",
    "    })\n",
    "\n",
    "    return output_sarcasm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'neutral', 'score': 0.8819799423217773}, {'label': 'positive', 'score': 0.10076777637004852}, {'label': 'negative', 'score': 0.01725233532488346}]]\n",
      "{'error': 'Model helinivan/english-sarcasm-detector is currently loading', 'estimated_time': 20.0}\n"
     ]
    }
   ],
   "source": [
    "#TESTING STUFF\n",
    "print(sentiment_detection(\"I just loveeee stinky smelly stuff\"))\n",
    "print(sarcasm_detection(\"I just loveeee stinky smelly stuff\"))\n",
    "test_1 = sentiment_detection(\"I just loveeee stinky smelly stuff\")\n",
    "test_2 = sarcasm_detection(\"I just loveeee stinky smelly stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'negative', 'score': 0.6726911664009094}, {'label': 'neutral', 'score': 0.18366330862045288}, {'label': 'positive', 'score': 0.14364556968212128}]]\n",
      "0.6726911664009094\n",
      "0.18366330862045288\n",
      "0.14364556968212128\n"
     ]
    }
   ],
   "source": [
    "print(test_1)\n",
    "negative_score = test_1[0][0]['score']\n",
    "neutral_score = test_1[0][1]['score']\n",
    "positive_score = test_1[0][2]['score']\n",
    "\n",
    "\n",
    "print(negative_score)\n",
    "print(neutral_score)\n",
    "print(positive_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.8405861854553223}, {'label': 'LABEL_1', 'score': 0.15941382944583893}]]\n",
      "0.8405861854553223\n",
      "0.15941382944583893\n"
     ]
    }
   ],
   "source": [
    "print(test_2)\n",
    "sarcastic = test_2[0][0]['score']\n",
    "not_sarcastic = test_2[0][1]['score']\n",
    "\n",
    "\n",
    "print(sarcastic)\n",
    "print(not_sarcastic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the api calls to the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time # To add a delay between API calls\n",
    "\n",
    "def process_tweets_1(input_file, output_file):\n",
    "    # Open the input CSV file for reading and the output CSV file for writing\n",
    "    with open(input_file, 'r', newline='') as file_in, open(output_file, 'w', newline='') as file_out:\n",
    "        reader = csv.DictReader(file_in)\n",
    "\n",
    "        # Define the fieldnames for the output CSV file\n",
    "        fieldnames = reader.fieldnames + ['negative', 'neutral', 'positive', 'sarcastic', 'not_sarcastic']\n",
    "        \n",
    "        writer = csv.DictWriter(file_out, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Read and process each row in the input CSV file\n",
    "        for row in reader:\n",
    "            tweet_text = row['text']  # Assuming the tweet text is in the 'text' column\n",
    "\n",
    "            # Call the sentiment detection and sarcasm detection functions\n",
    "            sentiment_result = sentiment_detection(tweet_text)\n",
    "            sarcasm_result = sarcasm_detection(tweet_text)\n",
    "\n",
    "\n",
    "            # Add the sentiment and sarcasm probabilities to the row\n",
    "            row['negative'] = sentiment_result[0][0]['score']\n",
    "            row['neutral'] = sentiment_result[0][1]['score']\n",
    "            row['positive'] = sentiment_result[0][2]['score']\n",
    "            row['sarcastic'] = sarcasm_result[0][0]['score']\n",
    "            row['not_sarcastic'] = sarcasm_result[0][1]['score']\n",
    "\n",
    "            # Write the updated row to the output CSV file\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            # Delay for one minute before making the next API call\n",
    "            time.sleep(60)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "MAX_RETRY_COUNT = 3\n",
    "RATE_LIMIT_ERROR_MESSAGE = 'Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate'\n",
    "\n",
    "def process_tweets(input_file, output_file, log_file):\n",
    "    # Open the input CSV file for reading, the output CSV file for writing, and the log CSV file for writing unsuccessful runs\n",
    "    with open(input_file, 'r', newline='') as file_in, open(output_file, 'w', newline='') as file_out, open(log_file, 'w', newline='') as file_log:\n",
    "        reader = csv.DictReader(file_in)\n",
    "\n",
    "        # Define the fieldnames for the output CSV file\n",
    "        fieldnames = reader.fieldnames + ['negative', 'neutral', 'positive', 'sarcastic', 'not_sarcastic']\n",
    "        \n",
    "        writer = csv.DictWriter(file_out, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Define the fieldnames for the log CSV file\n",
    "        log_fieldnames = reader.fieldnames\n",
    "        log_writer = csv.DictWriter(file_log, fieldnames=log_fieldnames)\n",
    "        log_writer.writeheader()\n",
    "\n",
    "        # Read and process each row in the input CSV file\n",
    "        for row in reader:\n",
    "            tweet_text = row['text']  # Assuming the tweet text is in the 'text' column\n",
    "            retry_count = 0\n",
    "\n",
    "            while retry_count < MAX_RETRY_COUNT:\n",
    "                # Call the sentiment detection and sarcasm detection functions\n",
    "                sentiment_result = sentiment_detection(tweet_text)\n",
    "                sarcasm_result = sarcasm_detection(tweet_text)\n",
    "\n",
    "                if sentiment_result and sarcasm_result:  # Check if results are not empty\n",
    "                    if (isinstance(sentiment_result[0], dict) and 'error' in sentiment_result[0] and sentiment_result[0]['error'] == RATE_LIMIT_ERROR_MESSAGE) or (isinstance(sarcasm_result[0], dict) and 'error' in sarcasm_result[0] and sarcasm_result[0]['error'] == RATE_LIMIT_ERROR_MESSAGE):\n",
    "                        # Log the row if rate limit error is encountered\n",
    "                        log_writer.writerow(row)\n",
    "                    else:\n",
    "                        # Add the sentiment and sarcasm probabilities to the row\n",
    "                        row['negative'] = sentiment_result[0][0]['score']\n",
    "                        row['neutral'] = sentiment_result[0][1]['score']\n",
    "                        row['positive'] = sentiment_result[0][2]['score']\n",
    "                        row['sarcastic'] = sarcasm_result[0][0]['score']\n",
    "                        row['not_sarcastic'] = sarcasm_result[0][1]['score']\n",
    "\n",
    "                        # Write the updated row to the output CSV file\n",
    "                        writer.writerow(row)\n",
    "                    break\n",
    "                else:\n",
    "                    # Log the unsuccessful run\n",
    "                    log_writer.writerow(row)\n",
    "\n",
    "                    retry_count += 1\n",
    "                    time.sleep(60)  # Delay for one minute before making the next retry\n",
    "\n",
    "            if retry_count == MAX_RETRY_COUNT:\n",
    "                # If maximum retry count reached, write the original row to the output CSV file\n",
    "                writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m output_csv_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpreprocessed_test.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m log_csv_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlog.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m process_tweets(input_csv_file, output_csv_file, log_csv_file)\n",
      "Cell \u001b[1;32mIn[52], line 34\u001b[0m, in \u001b[0;36mprocess_tweets\u001b[1;34m(input_file, output_file, log_file)\u001b[0m\n\u001b[0;32m     31\u001b[0m sarcasm_result \u001b[39m=\u001b[39m sarcasm_detection(tweet_text)\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m sentiment_result \u001b[39mand\u001b[39;00m sarcasm_result:  \u001b[39m# Check if results are not empty\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(sentiment_result[\u001b[39m0\u001b[39;49m], \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m sentiment_result[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m sentiment_result[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m RATE_LIMIT_ERROR_MESSAGE) \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(sarcasm_result[\u001b[39m0\u001b[39m], \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m sarcasm_result[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m sarcasm_result[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m RATE_LIMIT_ERROR_MESSAGE):\n\u001b[0;32m     35\u001b[0m         \u001b[39m# Log the row if rate limit error is encountered\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         log_writer\u001b[39m.\u001b[39mwriterow(row)\n\u001b[0;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[39m# Add the sentiment and sarcasm probabilities to the row\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "input_csv_file = 'test.csv'\n",
    "output_csv_file = 'preprocessed_test.csv'\n",
    "log_csv_file = 'log.csv'\n",
    "\n",
    "process_tweets(input_csv_file, output_csv_file, log_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "input_csv_file = 'train.csv'\n",
    "output_csv_file = 'preprocessed_train.csv'\n",
    "\n",
    "process_tweets(input_csv_file, output_csv_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features in a DataFrame to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def convert_features_to_one_hot(df, feature_name_list):\n",
    "  for feature_name in feature_name_list:\n",
    "    df = pd.get_dummies(df, columns=[feature_name])\n",
    "  \n",
    "  return df\n",
    "#Define the training set/test set from the imported data... x_train, x_val, etc... needs to be predefined\n",
    "#data_to_convert = ['x_train', 'x_val', 'x_test']\n",
    "data_to_convert = [] #get rid of this after that task is complete\n",
    "feature_list = ['location', 'keyword']\n",
    "for i,ix in enumerate(data_to_convert):\n",
    "  exec(f'{data_to_convert[i]} = convert_features_to_one_hot({ix}, {feature_list}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSV's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('preprocessed_train.csv', sep=',', encoding='latin-1')\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   keyword  location  sarcastic  not_sarcastic  negative   neutral  positive\n",
      "8      NaN       NaN   0.991654       0.008346  0.863112  0.093495  0.043393\n",
      "15     NaN       NaN   0.581560       0.418440  0.805112  0.160103  0.034786\n",
      "36  ablaze  Pretoria   0.987365       0.012635  0.637389  0.328306  0.034305\n",
      "0      NaN       NaN   0.990780       0.009220  0.800202  0.180087  0.019711\n",
      "11     NaN       NaN   0.889461       0.110539  0.684063  0.245803  0.070134\n",
      "33  ablaze    AFRICA   0.994544       0.005456  0.621827  0.366734  0.011439\n",
      "21     NaN       NaN   0.992460       0.007540  0.962976  0.032660  0.004363\n",
      "28     NaN       NaN   0.886264       0.113736  0.921403  0.071856  0.006741\n"
     ]
    }
   ],
   "source": [
    "X_data = train_df[['keyword', 'location', 'sarcastic', 'not_sarcastic', 'negative', 'neutral', 'positive']]\n",
    "Y_data = train_df[\"target\"]\n",
    "#print(X_data[\"neutral\"])\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size = 0.2)\n",
    "print(X_val)\n",
    "# split the dataset into training and validation sets\n",
    "#train_data, val_data = \n",
    "#print(train_data[:][\"sarcastic\"])\n",
    "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model draft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4233, 0.5767]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class netmodel(nn.Module):\n",
    "  def __init__(self, input_layer=1, num_hidden=1, node_per_hidden=32, droppout=0., LSTM_layers=0, outputs=2):\n",
    "    super(netmodel, self).__init__()\n",
    "    self.input_layer = input_layer\n",
    "    self.num_hidden = num_hidden \n",
    "    self.node_per_hidden = node_per_hidden\n",
    "    self.droppout = droppout \n",
    "    self.SLTM_layers = LSTM_layers \n",
    "    self.outputs = outputs \n",
    "    self.inputfc = nn.Linear(input_layer, node_per_hidden) \n",
    "    self.hiddenfc = [] \n",
    "    for i in range(num_hidden-1):\n",
    "      self.hiddenfc.append(nn.Linear(node_per_hidden, node_per_hidden))\n",
    "    self.lastfc = nn.Linear(node_per_hidden, outputs)\n",
    "\n",
    "  def forward(self, x, debug=False):\n",
    "    drop = nn.Dropout(p=self.droppout)\n",
    "    x = x.view(1,1)\n",
    "    x = self.inputfc(x)\n",
    "    x = F.relu(x)\n",
    "    x = drop(x)\n",
    "    for i in range(self.num_hidden-1):\n",
    "      x = self.hiddenfc[i](x)\n",
    "      x = F.relu(x)\n",
    "      x = drop(x)\n",
    "    \n",
    "    x = self.lastfc(x)\n",
    "    x = F.softmax(x, dim=1)\n",
    "    return x \n",
    "\n",
    "\n",
    "\n",
    "Model = netmodel(input_layer=7, num_hidden=3, droppout=0.1).to(device)\n",
    "\n",
    "Model.forward(torch.tensor([[1., 1., 1., 1., 1., 1., 1.]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
