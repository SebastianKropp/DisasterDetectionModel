{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Disaster Detection Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "! pip3 install -r requirements.txt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "###              EXAMPLE USE:                ###\n",
    "## tweet_text = \"This is a test tweet!\"\n",
    "## sentiment = sentiment_detection(tweet_text)\n",
    "## sarcasm = sarcasm_detection(tweet_text) \n",
    "API_TOKEN = \"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\"\n",
    "\n",
    "def sentiment_detection(tweet_text):\n",
    "    # Define the first API endpoint and function\n",
    "    API_URL_1 = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    headers_1 = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    def query_sentiment(payload):\n",
    "        response = requests.post(API_URL_1, headers=headers_1, json=payload)\n",
    "        return response.json()\n",
    "\n",
    "    # Use the first function to query the sentiment of some text\n",
    "    output_sentiment = query_sentiment({\n",
    "        \"inputs\": tweet_text\n",
    "    })\n",
    "\n",
    "\n",
    "def sarcasm_detection(tweet_text):\n",
    "    # Define the second API endpoint and function\n",
    "    API_URL_2 = \"https://api-inference.huggingface.co/models/helinivan/english-sarcasm-detector\"\n",
    "    headers_2 = {\"Authorization\": \"Bearer {API_TOKEN}\"}\n",
    "\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL_2, headers_2=headers_2, json=payload)\n",
    "        return response.json()\n",
    "        \n",
    "    output = query({\n",
    "        \"inputs\": tweet_text,\n",
    "    })\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features in a DataFrame to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def convert_features_to_one_hot(df, feature_name_list):\n",
    "  for feature_name in feature_name_list:\n",
    "    df = pd.get_dummies(df, columns=[feature_name])\n",
    "  \n",
    "  return df\n",
    "#Define the training set/test set from the imported data... x_train, x_val, etc... needs to be predefined\n",
    "#data_to_convert = ['x_train', 'x_val', 'x_test']\n",
    "data_to_convert = [] #get rid of this after that task is complete\n",
    "feature_list = ['location', 'keyword']\n",
    "for i,ix in enumerate(data_to_convert):\n",
    "  exec(f'{data_to_convert[i]} = convert_features_to_one_hot({ix}, {feature_list}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
