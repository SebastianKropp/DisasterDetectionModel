{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "collapsed_sections": [
        "2YI-tk0dFuMj"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SebastianKropp/DisasterDetectionModel/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSE 144 Spring 2023 Project\n",
        "\n",
        "Transformer model for disaster tweet detection.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Preprocessing: Preprocess the text by tokenizing and converting the text to numerical representations using tokenizer from pretrained model.\n",
        "\n",
        "2. Model Implementation: Implement a BERT model for detecting disaster tweets.\n",
        "\n",
        "3. Model Evaluation: Train the model on the preprocessed dataset and evaluate its performance using accuracy, precision, recall, and F1-score. Experiment with different hyperparameters such as learning rate, batch size, and number of epochs to optimize the model's performance.\n",
        "\n",
        "4. (Optional) Model Interpretation: Analyze the model's predictions by examining the most important features or words that contribute to a disaster or non-disaster tweet. (Optional: Visualize the results using techniques such as word clouds or attention maps).\n",
        "\n",
        "Datasets: \n",
        "\n",
        "- Kaggle dataset: https://www.kaggle.com/competitions/nlp-getting-started/data\n",
        "- Additional: https://www.kaggle.com/datasets/vstepanenko/disaster-tweets?select=tweets.csv"
      ],
      "metadata": {
        "id": "Pc2_kdXYepaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies and Imports"
      ],
      "metadata": {
        "id": "J32lfKz8wX8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece sacremoses importlib_metadata\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "DGPnvGcSvOQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jbtuPaYOedkp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import math\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing\n",
        "Preprocess the text by tokenizing and converting the text to numerical representations."
      ],
      "metadata": {
        "id": "D6KweNTHfi-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "XaXJAW0tmXHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from CSV\n",
        "dataset_path = 'preprocessed_train.csv'  # Replace with the path to your dataset\n",
        "dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "# Combine the additional columns into a feature matrix\n",
        "features = dataset[['neutral', 'negative', 'sarcastic']].values\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels, train_features, test_features = train_test_split(dataset['text'], dataset['target'], features, test_size=0.2, random_state=42)\n",
        "\n",
        "# Specify the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "model_name = 'prajjwal1/bert-tiny'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "PZ7ylaGmfnbk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "hlwpPBLUlIqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts, tokenizer):\n",
        "  texts = texts.tolist() if isinstance(texts, pd.Series) else texts  # Convert to list if needed\n",
        "\n",
        "  encoded_inputs = tokenizer(\n",
        "    texts,\n",
        "    padding='longest',\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors='pt',\n",
        "    add_special_tokens=True\n",
        "  )\n",
        "  input_ids = encoded_inputs['input_ids'].to(device)\n",
        "  attention_masks = encoded_inputs['attention_mask'].to(device)\n",
        "\n",
        "  return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "FtdXxzdylBNT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test preprocess()\n",
        "test = train_texts[:2]\n",
        "input_ids, attention_masks = preprocess(test, tokenizer)\n",
        "\n",
        "# Print the first example\n",
        "print(\"Text:\", test.iloc[0])\n",
        "print(\"Tokenized input:\", tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))\n",
        "print(\"Input IDs:\", input_ids[0])\n",
        "print(\"Attention Mask:\", attention_masks[0])"
      ],
      "metadata": {
        "id": "7EWEfgcnNNiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build DataLoaders"
      ],
      "metadata": {
        "id": "eBb55WNoIStj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Build train dataset -----\n",
        "\n",
        "# Preprocess the text data\n",
        "train_input_ids, train_attention_masks = preprocess(train_texts, tokenizer)\n",
        "\n",
        "# Convert train_labels to a list, then to a tensor\n",
        "train_labels_tensor = torch.tensor(train_labels.tolist()).to(device)\n",
        "\n",
        "# Build train dataset\n",
        "train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels_tensor)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# ------ Build test dataset -----\n",
        "\n",
        "# Preprocess the text data\n",
        "test_input_ids, test_attention_masks = preprocess(test_texts, tokenizer)\n",
        "\n",
        "# Convert train_labels to a list, then to a tensor\n",
        "test_labels_tensor = torch.tensor(test_labels.tolist()).to(device)\n",
        "\n",
        "# Build test dataset\n",
        "test_dataset = torch.utils.data.TensorDataset(test_input_ids, test_attention_masks, test_labels_tensor)"
      ],
      "metadata": {
        "id": "cwhluJ_MR--3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Build DataLoaders\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Oxg2sNBvIWlW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "Implement model using a pre-trained model."
      ],
      "metadata": {
        "id": "2dJtHHPJfoHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture captured_output\n",
        "\n",
        "# Number of labels\n",
        "num_labels = 2\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# Download model and configuration from S3 and cache.\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "last_state_dim = config.hidden_size\n",
        "model.classifier = torch.nn.Linear(last_state_dim, num_labels) # Replace the pre-trained head with a new one.\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "del captured_output"
      ],
      "metadata": {
        "id": "dqLuEZHEfrBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d91715-3ad5-4888-bfd5-a38b0f43c23e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "Train the model on the preprocessed dataset and evaluate its performance using accuracy, precision, recall, and F1-score. Experiment with different hyperparameters such as learning rate, batch size, and number of epochs to optimize the model's performance."
      ],
      "metadata": {
        "id": "19sDaTXRfrmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "pkMT8YPwygmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, trainloader, valloader):\n",
        "  model.train()\n",
        "\n",
        "  losses = []  # List to store the loss after each epoch\n",
        "  accuracies = []\n",
        "  val_accuracies = []\n",
        "  best_val_accuracy = -1  # Initialize the best validation accuracy\n",
        "  early_stopping_counter = 0  # Counter for early stopping\n",
        "    \n",
        "  for epoch in range(epochs):\n",
        "      running_loss = 0.0\n",
        "        \n",
        "      for batch in trainloader:\n",
        "          input_ids, attention_masks, labels = batch\n",
        "          input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
        "            \n",
        "          optimizer.zero_grad()\n",
        "            \n",
        "          # Forward pass\n",
        "          outputs = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "          if hasattr(outputs, 'logits'):\n",
        "              # BERT model\n",
        "              logits = outputs.logits.to(device)\n",
        "              loss = criterion(logits, labels)\n",
        "          else:\n",
        "              # GPT model\n",
        "              #hidden_states = outputs.last_hidden_state.to(device)\n",
        "              #logits = model.classifier(hidden_states)\n",
        "              #labels = F.one_hot(labels, num_labels).float().to(device)    \n",
        "              logits = outputs.last_hidden_state[:, 0, :]          \n",
        "              loss = criterion(logits, labels.long())\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "          optimizer.step()\n",
        "          #scheduler.step()\n",
        "          losses.append(loss.detach().cpu().numpy())\n",
        "\n",
        "          # Calculate accuracy\n",
        "          _, predicted_labels = torch.max(logits, dim=1)\n",
        "          batch_accuracy = (predicted_labels == labels).sum().item() / labels.size(0)\n",
        "          accuracies.append(batch_accuracy)\n",
        "\n",
        "          # Uncomment to calculate validation accuracy (takes more time)\n",
        "          val_accuracy, _, _, _, _ = evaluate_model(model, valloader, False)\n",
        "          val_accuracies.append(val_accuracy)\n",
        "\n",
        "      # Check if the current validation accuracy is better than the previous best\n",
        "      if val_accuracy > best_val_accuracy:\n",
        "          best_val_accuracy = val_accuracy\n",
        "          early_stopping_counter = 0  # Reset the counter\n",
        "      else:\n",
        "          early_stopping_counter += 1  # Increment the counter\n",
        "\n",
        "      print('Train Epoch: {}\\tLoss: {:.6f}\\tTrain Accuracy: {:.4f}\\tValidation Accuracy: {:.4f}'.format(\n",
        "        epoch, loss.item(), batch_accuracy, val_accuracy))\n",
        "      #print('Train Epoch: {}\\tLoss: {:.6f}\\tTrain Accuracy: {:.4f}'.format(\n",
        "      #  epoch, loss.item(), batch_accuracy))\n",
        "\n",
        "      # Check if early stopping criterion is met\n",
        "      if early_stopping_counter >= patience:\n",
        "        print(\"Early stopping triggered. No improvement in validation accuracy for {} epochs.\".format(patience))\n",
        "        break\n",
        "\n",
        "  return losses, accuracies, val_accuracies"
      ],
      "metadata": {
        "id": "GmBet-H5IZHH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rate and number of epochs\n",
        "learning_rate = 1e-5\n",
        "epochs = 50\n",
        "patience = 2\n",
        "\n",
        "# Define the optimizer and criterion\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # best learning rate: 1e-5, avg result: ~80%\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=learning_rate) # best learning rate: 1e-5, avg result: ~81% accuracy\n",
        "#optimizer = optim.Adam(model.parameters(), lr=learning_rate) # best learning rate: 1e-5, avg result: ~80% accuracy\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\n",
        "\n",
        "# Define the scheduler to adjust learning rate\n",
        "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(trainloader) * epochs)"
      ],
      "metadata": {
        "id": "RA1qFhRrfvgb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "losses, accuracies, val_accuracies = train_model(model, optimizer, criterion, trainloader, valloader)"
      ],
      "metadata": {
        "id": "r2gfCxVdIoC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82e034a-f26e-4a7c-e802-a487a6606f3b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0\tLoss: 0.596372\tTrain Accuracy: 0.8333\tValidation Accuracy: 0.6601\n",
            "Train Epoch: 1\tLoss: 0.460056\tTrain Accuracy: 1.0000\tValidation Accuracy: 0.7184\n",
            "Train Epoch: 2\tLoss: 0.634165\tTrain Accuracy: 0.8333\tValidation Accuracy: 0.7356\n",
            "Train Epoch: 3\tLoss: 0.387445\tTrain Accuracy: 0.8333\tValidation Accuracy: 0.7529\n",
            "Train Epoch: 4\tLoss: 0.674595\tTrain Accuracy: 0.5000\tValidation Accuracy: 0.7685\n",
            "Train Epoch: 5\tLoss: 0.581835\tTrain Accuracy: 0.6667\tValidation Accuracy: 0.7808\n",
            "Train Epoch: 6\tLoss: 0.410823\tTrain Accuracy: 0.6667\tValidation Accuracy: 0.7800\n",
            "Train Epoch: 7\tLoss: 0.418692\tTrain Accuracy: 0.8333\tValidation Accuracy: 0.7808\n",
            "Early stopping triggered. No improvement in validation accuracy for 2 epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare train and validation accuracy during training\n",
        "plt.plot(accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train and Validation Accuracies')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j6PbBXEY2JVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(range(0, len(losses)), losses)"
      ],
      "metadata": {
        "id": "xLR2mw0xjWzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.plot(range(0, len(accuracies)), accuracies)"
      ],
      "metadata": {
        "id": "IL5CXiN2s6rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "BkezpdAdyWAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, is_test=True):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    predicted_probs = []\n",
        "    input_ids_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_masks, labels = batch\n",
        "            input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "            if hasattr(outputs, 'logits'):\n",
        "                # BERT model\n",
        "                logits = outputs.logits.to(device)\n",
        "            else:\n",
        "                # GPT model\n",
        "                #hidden_states = outputs.last_hidden_state\n",
        "                #logits = model.classifier(hidden_states)\n",
        "                logits = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "            _, predicted_labels = torch.max(logits, dim=1)\n",
        "            predicted_prob = torch.softmax(logits, dim=1)\n",
        "\n",
        "            predictions.extend(predicted_labels.cpu().tolist())\n",
        "            #predictions.extend(F.one_hot(predicted_labels, num_labels).float().cpu().tolist())\n",
        "            true_labels.extend(labels.cpu().tolist())\n",
        "            #true_labels.extend(F.one_hot(labels, num_labels).float().cpu().tolist())\n",
        "            predicted_probs.extend(predicted_prob.cpu().tolist())  # Assuming class 1 is the positive class\n",
        "            input_ids_list.extend(input_ids.cpu().tolist())\n",
        "\n",
        "    predicted_probs = np.array(predicted_probs)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "    if is_test:\n",
        "      print(\"Evaluation on test set:\")\n",
        "      precision = precision_score(true_labels, predictions, average='macro')\n",
        "      recall = recall_score(true_labels, predictions, average='macro')\n",
        "      f1 = f1_score(true_labels, predictions, average='macro')\n",
        "      return accuracy, precision, recall, f1, true_labels, predictions, predicted_probs, input_ids_list\n",
        "\n",
        "    else:\n",
        "        return accuracy, true_labels, predictions, predicted_probs, input_ids_list"
      ],
      "metadata": {
        "id": "DFdV6eVjQdQh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "accuracy, precision, recall, f1, true_labels, predictions, predicted_probs, input_ids_list = evaluate_model(model, testloader)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "v2BWeswWQht_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4779ef1d-7a1f-4554-f741-80cb2b8e9f60"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set:\n",
            "Accuracy: 0.7858\n",
            "Precision: 0.7829\n",
            "Recall: 0.7775\n",
            "F1 Score: 0.7796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_significant_words(input_ids_list, predicted_probs):\n",
        "    token_scores = {}\n",
        "\n",
        "    # Aggregate the predicted probabilities for each token across examples\n",
        "    for example_probs in predicted_probs:\n",
        "        for token_index, prob_positive_class in enumerate(example_probs):\n",
        "            if token_index not in token_scores:\n",
        "                token_scores[token_index] = prob_positive_class\n",
        "            else:\n",
        "                token_scores[token_index] += prob_positive_class\n",
        "\n",
        "    # Sort the token scores in descending order\n",
        "    sorted_scores = sorted(token_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top 10 most significant words\n",
        "    top_10_tokens = [token_index for token_index, _ in sorted_scores[:10]]\n",
        "    top_10_words = [tokenizer.convert_ids_to_tokens(input_ids[token_index]) for token_index in top_10_tokens]\n",
        "\n",
        "    return top_10_words"
      ],
      "metadata": {
        "id": "-OioxU556TSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = find_most_significant_words(input_ids_list, predicted_probs)\n",
        "print(top_words)"
      ],
      "metadata": {
        "id": "6oUkJcKE6YRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "mHZ4oO0Qh-tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, roc_auc_score\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "zZ0_VY_XiB1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(true_labels, predicted_labels):\n",
        "    # Calculate the confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    # Create a heatmap for the confusion matrix\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "    # Set axis labels and title\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(true_labels, predictions)"
      ],
      "metadata": {
        "id": "eNpnZVE9uMq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(true_labels, predicted_probs):\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, predicted_probs[:, 1])\n",
        "    auc = roc_auc_score(true_labels, predicted_probs[:, 1])\n",
        "\n",
        "    plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc))\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='r', label='Random Guess')\n",
        "    plt.xlabel('False Positive Rate (FPR)')\n",
        "    plt.ylabel('True Positive Rate (TPR)')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_roc_curve(true_labels, predicted_probs)"
      ],
      "metadata": {
        "id": "D6iaq_5Cw2_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT"
      ],
      "metadata": {
        "id": "RILD7_SLcAul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model, GPT2Tokenizer, GPT2Config, AutoModelForCausalLM, GPT2LMHeadModel "
      ],
      "metadata": {
        "id": "4OUIM4iUcDCQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "model_name_gpt = 'distilgpt2'\n",
        "model_config_gpt = GPT2Config.from_pretrained(model_name_gpt)\n",
        "\n",
        "#device = 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "8y8OqJmkcG_X"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained(model_name_gpt)\n",
        "tokenizer_gpt.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model_config_gpt.vocab_size = len(tokenizer_gpt)\n",
        "\n",
        "gpt = GPT2Model.from_pretrained(model_name_gpt, config=model_config_gpt, ignore_mismatched_sizes=True)\n",
        "\n",
        "# Replace the pre-trained head with a new one\n",
        "last_state_dim_gpt = model_config_gpt.hidden_size\n",
        "gpt.classifier = nn.Linear(last_state_dim_gpt, num_labels)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "gpt.to(device)\n",
        "\n",
        "#assert tokenizer_gpt.pad_token_id == gpt.config.pad_token_id\n",
        "\n",
        "# Build train dataset\n",
        "train_input_ids_gpt, train_attention_masks_gpt = preprocess(train_texts, tokenizer_gpt)\n",
        "train_labels_gpt = torch.tensor(train_labels.tolist()).to(device)\n",
        "train_dataset_gpt = torch.utils.data.TensorDataset(train_input_ids_gpt, train_attention_masks_gpt, train_labels_gpt)\n",
        "\n",
        "# Split the dataset into train and validation sets\n",
        "train_size_gpt = int(0.8 * len(train_dataset_gpt))\n",
        "val_size_gpt = len(train_dataset_gpt) - train_size_gpt\n",
        "train_dataset_gpt, val_dataset_gpt = torch.utils.data.random_split(train_dataset_gpt, [train_size_gpt, val_size_gpt])\n",
        "\n",
        "# Build test dataset\n",
        "test_input_ids_gpt, test_attention_masks_gpt = preprocess(test_texts, tokenizer_gpt)\n",
        "test_labels_gpt = torch.tensor(test_labels.tolist()).to(device)\n",
        "test_dataset_gpt = torch.utils.data.TensorDataset(test_input_ids_gpt, test_attention_masks_gpt, test_labels_gpt)\n",
        "\n",
        "# Define batch size\n",
        "batch_size_gpt = 32\n",
        "\n",
        "# Build DataLoaders\n",
        "trainloader_gpt = torch.utils.data.DataLoader(train_dataset_gpt, batch_size=batch_size_gpt, shuffle=True)\n",
        "testloader_gpt = torch.utils.data.DataLoader(test_dataset_gpt, batch_size=batch_size_gpt, shuffle=False)\n",
        "valloader_gpt = torch.utils.data.DataLoader(val_dataset_gpt, batch_size=batch_size_gpt, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGEghV4ddAEk",
        "outputId": "839a9049-3517-4d0d-a039-921be77dcf34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2Model were not initialized from the model checkpoint at distilgpt2 and are newly initialized because the shapes did not match:\n",
            "- transformer.wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([50258, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rate and number of epochs\n",
        "learning_rate_gpt = 1e-5\n",
        "epochs_gpt = 50\n",
        "patience_gpt = 2\n",
        "\n",
        "# Define the optimizer and criterion\n",
        "optimizer_gpt = optim.AdamW(gpt.parameters(), lr=learning_rate_gpt)\n",
        "criterion_gpt = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "U4G3nrKrggBq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "losses_gpt, accuracies_gpt, val_accuracies_gpt = train_model(gpt, optimizer_gpt, criterion_gpt, trainloader_gpt, valloader_gpt)"
      ],
      "metadata": {
        "id": "MT7-rLzpgjCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "accuracy_gpt, precision_gpt, recall_gpt, f1_gpt, true_labels_gpt, predictions_gpt, predicted_probs_gpt, input_ids_list_gpt = evaluate_model(gpt, testloader_gpt)"
      ],
      "metadata": {
        "id": "YKAZULhtgmgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "2YI-tk0dFuMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "u2nRFa2tWnCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(max_iter=8000, solver='lbfgs')"
      ],
      "metadata": {
        "id": "h-PojoOoWd2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(text):\n",
        "    # Mean word length\n",
        "    words = text.split()\n",
        "    mean_word_length = sum(len(word) for word in words) / len(words)\n",
        "\n",
        "    # Character count\n",
        "    character_count = len(text)\n",
        "\n",
        "    # Punctuation count\n",
        "    punctuation_count = len(re.findall(r'[^\\w\\s]', text))\n",
        "\n",
        "    return mean_word_length, character_count, punctuation_count"
      ],
      "metadata": {
        "id": "Y5Ls3aQ4WgeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature extraction to the train and test texts\n",
        "train_features_text = [extract_features(text) for text in train_texts]\n",
        "test_features_text = [extract_features(text) for text in test_texts]\n",
        "\n",
        "# Convert the features to a NumPy array and standardize the values if needed\n",
        "train_features_text = np.array(train_features_text)\n",
        "test_features_text = np.array(test_features_text)\n",
        "\n",
        "# Concatenate the text features and additional features\n",
        "train_features_all = np.concatenate((train_features_text, train_features), axis=1)\n",
        "test_features_all = np.concatenate((test_features_text, test_features), axis=1)\n",
        "\n",
        "# Scale the input features\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features_all)\n",
        "test_features_scaled = scaler.transform(test_features_all)"
      ],
      "metadata": {
        "id": "QmiiQSNkWugO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"Train Feature\", i + 1)\n",
        "    print(train_features_all[i])\n",
        "    print()"
      ],
      "metadata": {
        "id": "UGZf2DI0X8L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train logistic regression model\n",
        "logreg.fit(train_features, train_labels)\n",
        "\n",
        "# Make predictions using logistic regression model\n",
        "logreg_predictions = logreg.predict(test_features)\n",
        "\n",
        "# Evaluate logistic regression model\n",
        "logreg_accuracy = accuracy_score(test_labels, logreg_predictions)\n",
        "print(\"Logistic Regression Accuracy:\", logreg_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvSc4IKyF0SP",
        "outputId": "b664e51b-c16e-478d-a9da-b7b5aeec6719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.6366622864651774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine results"
      ],
      "metadata": {
        "id": "FYIrrTfLb6mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average the predicted probabilities from BERT and logistic regression\n",
        "logreg_probs = logreg.predict_proba(test_features_all)[:, 1]  # Extract the probabilities of the positive class\n",
        "logreg_probs = logreg_probs.reshape(-1, 1)  # Reshape to match the shape of BERT's predicted probabilities\n",
        "combined_probs = (predicted_probs + logreg_probs) / 2\n",
        "\n",
        "# Choose the class with the highest average probability\n",
        "combined_predictions = np.argmax(combined_probs, axis=1)\n",
        "\n",
        "# Calculate the accuracy of the combined predictions\n",
        "combined_accuracy = accuracy_score(test_labels, combined_predictions)\n",
        "\n",
        "print('Combined Model Accuracy: {:.4f}'.format(combined_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwG38Vj1QaRd",
        "outputId": "11c59601-11fe-4370-a1a9-ad09eb50b7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined Model Accuracy: 0.7996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Disaster or Not\n",
        "\n",
        "Use trained model to predict if a tweet is about a disaster or not."
      ],
      "metadata": {
        "id": "iu3vAJFCkD0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_TA(text):\n",
        "  inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "  return inputs['input_ids'].to(device), inputs['attention_mask'].to(device)\n",
        "\n",
        "def predict_disaster_TA(texts):\n",
        "  model.eval() # Set the model to evaluation mode\n",
        "  input_ids, attention_mask = preprocess_TA(texts)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask) # Get the model outputs\n",
        "    logits = outputs.logits # Get the logits from the model outputs\n",
        "    probabilities = F.softmax(logits, dim=1)\n",
        "    disasters = torch.argmax(probabilities, dim=1).squeeze()\n",
        "\n",
        "  return disasters, probabilities"
      ],
      "metadata": {
        "id": "6L0UtFHEoYRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the predict_sentiment_TA function\n",
        "text = \"Was in NYC last week!\"\n",
        "disaster, probabilities = predict_disaster_TA(text)\n",
        "\n",
        "print(f\"Disaster: {disaster}\")\n",
        "print(f\"Probabilities: {probabilities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeOakJ-yLjlJ",
        "outputId": "513b7f03-e660-45a3-8ff6-7eb1a2d36df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disaster: 0\n",
            "Probabilities: tensor([[0.9047, 0.0953]], device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}